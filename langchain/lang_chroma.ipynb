{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24ed5288-2872-4bec-93bd-0445f875b504",
   "metadata": {},
   "source": [
    "## Below is the imports needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14fa9090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "import config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9fcf01-d947-4650-90dd-feeb4eb14a36",
   "metadata": {},
   "source": [
    "## Load documents\n",
    "Load documents to do question answering over. If you want to do this over your documents, this is the section you should replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "758acbb0-e595-4b05-a945-e7e774c18e7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"challenges.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a652694-9877-465f-9946-53bc0c8e9dc6",
   "metadata": {},
   "source": [
    "## Split documents\n",
    "\n",
    "Split documents into small chunks. This is so we can find the most relevant chunks for a query and pass only those into the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7cbbeef-4dbe-4c7d-a279-1f7946505235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "276b604c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d84594-38bf-4160-9be7-98b6ed3f6419",
   "metadata": {},
   "source": [
    "## Initialize ChromaDB\n",
    "\n",
    "Create embeddings for each chunk and insert into the Chroma vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fc55f4d-263d-4bb0-91f7-8740247c27f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "openai_api_key=config.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5719b62b-330f-4ab7-99cc-a631bbaaa33a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n",
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "vectordb = Chroma.from_documents(texts, embeddings)\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ee82a1-1d0f-431b-a407-54812b128b51",
   "metadata": {},
   "source": [
    "## Create the chain\n",
    "\n",
    "Initialize the chain we will use for question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fed3842-a683-43e5-a732-2a8922ff7130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(openai_api_key=openai_api_key), chain_type=\"stuff\", retriever=docsearch.as_retriever())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8259001d-9444-48df-b365-7261e26ebd79",
   "metadata": {},
   "source": [
    "## Ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c8c1dbc-1e7f-4adf-b6cc-09e15e941a87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The key point of this article is that companies should consider a single, integrated platform to achieve the highest business value for their data integration projects, rather than relying on a series of disparate tools.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the authors key point with this article?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839f0c8e-737c-4831-af8c-431c9b2fc9c3",
   "metadata": {},
   "source": [
    "Summarization involves creating a smaller summary of multiple longer documents. This can be useful for distilling long documents into the core pieces of information.\n",
    "\n",
    "The recommended way to get started using a summarization chain is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e615c917-8e99-4761-900e-ca80bff6f782",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import OpenAI, PromptTemplate, LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm=OpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "text_splitter = CharacterTextSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afa6d22a-5ce3-4b82-be2d-ad52d6f9ef65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This article discusses how to upgrade data architecture to make faster and smarter business decisions. It provides advice and guidance on leveraging modern data architectures, modular stacks, and pre-built connectors. It also covers important aspects such as global governance, vendor lock-in, and scalability, as well as discussing company-specific examples of successful data stack upgrades. It explains how to create a data-driven culture by introducing self-serve tools, promoting data innovation, and fostering collaboration between stakeholders. Finally, it introduces Mode and Sisu as advanced analytics solutions.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "loader = PyPDFLoader(\"Mode-2021-Modern-Data-Architecture.pdf\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "#docs = [Document(page_content=t) for t in texts[4:7]]\n",
    "chain.run(texts)\n",
    "#print(texts[4:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06f3a5-3264-4a35-8fc7-7a851c95a110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
