{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24ed5288-2872-4bec-93bd-0445f875b504",
   "metadata": {},
   "source": [
    "## Below is the imports needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14fa9090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9fcf01-d947-4650-90dd-feeb4eb14a36",
   "metadata": {},
   "source": [
    "## Load documents\n",
    "Load documents to do question answering over. If you want to do this over your documents, this is the section you should replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "758acbb0-e595-4b05-a945-e7e774c18e7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"Mode-2021-Modern-Data-Architecture.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a652694-9877-465f-9946-53bc0c8e9dc6",
   "metadata": {},
   "source": [
    "## Split documents\n",
    "\n",
    "Split documents into small chunks. This is so we can find the most relevant chunks for a query and pass only those into the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c7cbbeef-4dbe-4c7d-a279-1f7946505235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d84594-38bf-4160-9be7-98b6ed3f6419",
   "metadata": {},
   "source": [
    "## Initialize ChromaDB\n",
    "\n",
    "Create embeddings for each chunk and insert into the Chroma vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fc55f4d-263d-4bb0-91f7-8740247c27f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "openai_api_key='...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5719b62b-330f-4ab7-99cc-a631bbaaa33a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n",
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "vectordb = Chroma.from_documents(texts, embeddings)\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ee82a1-1d0f-431b-a407-54812b128b51",
   "metadata": {},
   "source": [
    "## Create the chain\n",
    "\n",
    "Initialize the chain we will use for question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fed3842-a683-43e5-a732-2a8922ff7130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(openai_api_key=openai_api_key), chain_type=\"stuff\", retriever=docsearch.as_retriever())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8259001d-9444-48df-b365-7261e26ebd79",
   "metadata": {},
   "source": [
    "## Ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c8c1dbc-1e7f-4adf-b6cc-09e15e941a87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' No, I cannot. This article is too long for me to summarize in four paragraphs.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Can you summarize this article in four paragraphs?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839f0c8e-737c-4831-af8c-431c9b2fc9c3",
   "metadata": {},
   "source": [
    "Summarization involves creating a smaller summary of multiple longer documents. This can be useful for distilling long documents into the core pieces of information.\n",
    "\n",
    "The recommended way to get started using a summarization chain is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e615c917-8e99-4761-900e-ca80bff6f782",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import OpenAI, PromptTemplate, LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm=OpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "text_splitter = CharacterTextSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "afa6d22a-5ce3-4b82-be2d-ad52d6f9ef65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This article provides businesses with advice on how to upgrade their data stack to make better and faster business decisions. It explains why modular data stacks are more scalable and flexible than monolithic solutions, and discusses how companies can future-proof their data strategies with modern data architectures. It also provides tips on when to improve a data stack, how to embed analytics, and how to build a strong data culture. Mode and Sisu are advanced analytics platforms to help businesses get to insights faster.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "loader = PyPDFLoader(\"Mode-2021-Modern-Data-Architecture.pdf\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "#docs = [Document(page_content=t) for t in texts[4:7]]\n",
    "chain.run(texts)\n",
    "#print(texts[4:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06f3a5-3264-4a35-8fc7-7a851c95a110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
